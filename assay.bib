
@inproceedings{sivathanu_astra:_2019,
	title = {Astra: Exploiting Predictability to Optimize Deep Learning},
	doi = {10.1145/3297858.3304072},
	pages = {909--923},
	author = {Sivathanu, Muthian and Chugh, Tapan and Singapuram, Sanjay Sri Vallabh and Zhou, Lidong},
	date = {2019}
}

@inproceedings{hua_boosting_2019,
	title = {Boosting the Performance of {CNN} Accelerators with Dynamic Fine-Grained Channel Gating},
	isbn = {978-1-4503-6938-1},
	doi = {10.1145/3352460.3358283},
	pages = {139--150},
	author = {Hua, Weizhe and Zhou, Yuan and Sa, Christopher and Zhang, Zhiru and Suh, G.},
	date = {2019}
}

@inproceedings{deng_tie:_2019,
	title = {{TIE}: energy-efficient tensor train-based inference engine for deep neural network},
	doi = {10.1145/3307650.3322258},
	pages = {264--278},
	author = {Deng, Chunhua and Sun, Fangxuan and Qian, Xuehai and Lin, Jun and Wang, Zhongfeng and Yuan, Bo},
	date = {2019}
}

@article{ma_neugraph:_2019,
	title = {{NeuGraph}: Parallel Deep Neural Network Computation on Large Graphs},
	pages = {443--458},
	author = {Ma, Lingxiao and Yang, Zhi and Miao, Youshan and Xue, Jilong and Wu, Ming and Zhou, Lidong and Dai, Yafei},
	date = {2019}
}

@inproceedings{frankle_lottery_2018,
	title = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
	url = {http://arxiv.org/abs/1803.03635},
	abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for {MNIST} and {CIFAR}10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
	author = {Frankle, Jonathan and Carbin, Michael},
	date = {2018},
	keywords = {deep-learning generalization readings sparsity}
}

@inproceedings{besta_slim_2019,
	location = {New York, {NY}, {USA}},
	title = {Slim Graph: Practical Lossy Graph Compression for Approximate Graph Processing, Storage, and Analytics},
	isbn = {978-1-4503-6229-0},
	url = {http://doi.acm.org/10.1145/3295500.3356182},
	doi = {10.1145/3295500.3356182},
	series = {{SC} '19},
	pages = {35:1--35:25},
	booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	publisher = {{ACM}},
	author = {Besta, Maciej and Weber, Simon and Gianinazzi, Lukas and Gerstenberger, Robert and Ivanov, Andrey and Oltchik, Yishai and Hoefler, Torsten},
	date = {2019},
	note = {event-place: Denver, Colorado}
}

@book{dhulipala_low-latency_2019,
	title = {Low-Latency Graph Streaming Using Compressed Purely-Functional Trees},
	author = {Dhulipala, Laxman and Shun, Julian and Blelloch, Guy},
	date = {2019}
}

@inproceedings{dong_network_2019,
	title = {Network Density of States},
	isbn = {978-1-4503-6201-6},
	doi = {10.1145/3292500.3330891},
	pages = {1152--1161},
	author = {Dong, Kun and Benson, Austin and Bindel, David},
	date = {2019}
}